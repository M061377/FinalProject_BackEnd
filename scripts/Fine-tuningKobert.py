import transformers
print("ğŸ¤– Transformers version:", transformers.__version__)

import os
import gc
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments
)
from sklearn.metrics import classification_report

# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
print("âœ… ë°ì´í„° ë¡œë”© ì¤‘...")
df = pd.read_csv('korean_emotion_dataset.csv')
label_encoder = LabelEncoder()
df['labels'] = label_encoder.fit_transform(df['label'])

train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    df['text'].tolist(), df['labels'].tolist(),
    test_size=0.3, random_state=42, stratify=df['labels']
)

val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels,
    test_size=0.5, random_state=42, stratify=temp_labels
)

train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels})
val_dataset = Dataset.from_dict({'text': val_texts, 'labels': val_labels})
test_dataset = Dataset.from_dict({'text': test_texts, 'labels': test_labels})

# 2. í† í¬ë‚˜ì´ì €
print("âœ… í† í¬ë‚˜ì´ì§• ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1")

def tokenize_function(examples):
    tokenized = tokenizer(
        examples['text'],
        padding="max_length",  # â† longestëŠ” ì‹¤ì œ ì—ëŸ¬ ìœ ë°œ ê°€ëŠ¥, max_lengthê°€ ì•ˆì „
        truncation=True,
        max_length=64
    )
    if 'token_type_ids' in tokenized:
        tokenized.pop('token_type_ids')  # KoBERT ì „ìš© ì²˜ë¦¬
    return tokenized

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# 3. ëª¨ë¸ ì´ˆê¸°í™”
model_path = "./kobert-finetuned-emotion-v4"
print("ğŸ§¼ ëª¨ë¸ ì´ˆê¸°í™” ë° fresh ì‹œì‘")
model = AutoModelForSequenceClassification.from_pretrained("skt/kobert-base-v1", num_labels=7)

# 4. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir=model_path,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,  # í•„ìš” ì‹œ ìª¼ê°œì„œ ì‹¤í–‰
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2
)

# 5. Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# 6. í•™ìŠµ
print("ğŸš€ 1íšŒì°¨ í•™ìŠµ ì‹œì‘")
trainer.train()

# 7. ëª¨ë¸ ì €ì¥
gc.collect()
torch.cuda.empty_cache()
model.save_pretrained(model_path, safe_serialization=False)
tokenizer.save_pretrained(model_path)

# 8. ì„±ëŠ¥ í‰ê°€
predictions = trainer.predict(val_dataset)
preds = predictions.predictions.argmax(-1)
labels = predictions.label_ids

print(classification_report(labels, preds, target_names=label_encoder.classes_))
print("âœ… 1íšŒì°¨ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
