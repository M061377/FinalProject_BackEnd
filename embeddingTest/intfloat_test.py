from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import torch
import json
import numpy as np

# ëª¨ë¸ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("intfloat/multilingual-e5-base")
model = AutoModel.from_pretrained("intfloat/multilingual-e5-base")

# ì±… ë°ì´í„° ë¡œë“œ
with open("for_embedding_test.json", "r", encoding="utf-8") as f:
    data = json.load(f)

titles = [book["title"] for book in data["books"]]
descriptions = [book["description"] for book in data["books"]]

# ì¿¼ë¦¬ ì…ë ¥ (ê³ ì •)
query = "ê³¼ê±°ë¥¼ ë°°ê²½ìœ¼ë¡œ í•œ ì•½ê°„ ë¬´ì„œìš´ ë‚´ìš©ì˜ í•œêµ­ ì¶”ë¦¬ ì†Œì„¤ ì¶”ì²œí•´ì¤˜"
query_input = "query: " + query
desc_inputs = ["passage: " + d for d in descriptions]

# ì „ì²´ ì„ë² ë”© ê³„ì‚°
inputs = tokenizer(
    [query_input] + desc_inputs, padding=True, truncation=True, return_tensors="pt"
)

with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)

query_emb = embeddings[0].unsqueeze(0)
desc_embs = embeddings[1:]

# ìœ ì‚¬ë„ ê³„ì‚°
scores = cosine_similarity(query_emb, desc_embs)[0]
top3_idx = np.argsort(scores)[::-1][:3]

# ê²°ê³¼ ì¶œë ¥
print(f"\nì…ë ¥ ë¬¸ì¥: {query}")
print("\nğŸ“š ìœ ì‚¬í•œ ì±… Top 3:")
for i, idx in enumerate(top3_idx, 1):
    print(f"{i}. {titles[idx]} (ìœ ì‚¬ë„: {scores[idx]:.4f})")
