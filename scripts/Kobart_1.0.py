import json
from transformers import (
    BartForConditionalGeneration, PreTrainedTokenizerFast
)

# 1. KoBART ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
kobart_model = BartForConditionalGeneration.from_pretrained("./kobart_finetuned")
kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("./kobart_finetuned")

# 2. ì…ë ¥ ë°ì´í„° ë¡œë“œ
with open("../data/bestseller_books.json", encoding="utf-8") as f:
    books = json.load(f)

# 3. ìš”ì•½ í•¨ìˆ˜
def summarize_kobart(text):
    input_ids = kobart_tokenizer.encode(text, return_tensors="pt", truncation=True, max_length=512)
    output = kobart_model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
    return kobart_tokenizer.decode(output[0], skip_special_tokens=True)

# 4. ìš”ì•½ ì‹¤í–‰
results = []
total = len(books)

for i, book in enumerate(books):
    title = book.get("title", f"Book {i+1}")
    full_desc = book.get("fullDescription", "").strip()

    if not full_desc:
        print(f"âš ï¸ [{i+1}/{total}] '{title}' fullDescription ë¹„ì–´ ìˆì–´ì„œ ê±´ë„ˆëœ€")
        continue

    print(f"ğŸ”„ ìš”ì•½ ì¤‘... [{i+1}/{total}] '{title}'")
    kobart_sum = summarize_kobart(full_desc)

    results.append({
        "title": title,
        "fullDescription": full_desc,
        "description_kobart": kobart_sum
    })

# 5. ê²°ê³¼ ì €ì¥
output_path = "../data/summary_result.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("ğŸ‰ KoBART ìš”ì•½ ì „ì²´ ì™„ë£Œ ë° ì €ì¥ë¨ â†’", output_path)
